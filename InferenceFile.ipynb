{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Including Libraries"
      ],
      "metadata": {
        "id": "itZnw5bWkmbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jfi6p2-PkdUx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import codecs, sys\n",
        "import random\n",
        "unlp = spacy.blank('ur')\n",
        "from collections import Counter\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "34f14gtLktBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Roman Urdu Dictionary\n",
        "##### -> This dictionary contains over 2000 roman urdu compound words."
      ],
      "metadata": {
        "id": "ZYMsQqyhk1hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadFromPickle(file):\n",
        "\n",
        "    with open(file, 'rb') as handle:\n",
        "        dictionary = pickle.load(handle)\n",
        "\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "DvLgbtJzkvve"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Roman Urdu Tokenizer"
      ],
      "metadata": {
        "id": "P6xRFm7Clgd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roman_tokenizer(sentence):\n",
        "\n",
        "    # Lowering\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Default Parameters\n",
        "    tokens = []\n",
        "    punctuation = '''!()%\\n٪-;۔،:\\n\\/'\"\\,“./؟_ء'''\n",
        "\n",
        "    dictionary =   loadFromPickle('./dictionary.pkl')\n",
        "    \n",
        "    sentence = unlp(sentence)\n",
        "\n",
        "    # Iterating Word By Word\n",
        "    for word in sentence:\n",
        "\n",
        "        word = str(word)\n",
        "        # Iterating Index By Index\n",
        "        for index in word:\n",
        "            \n",
        "              if index in punctuation:\n",
        "                    word = word.replace(index,'')\n",
        "\n",
        "        # Removing Any Remaining Special Characters\n",
        "        if word != \"\\n\" and word != \"\\r\" and word != \" \" and word != '' and word != \" \\r\" and word !='‘' and word !='’':\n",
        "              tokens.append(word)\n",
        "\n",
        "    bi_tokens = [] \n",
        "    for i in range(len(tokens)):\n",
        "        \n",
        "        if i + 1 < len(tokens):\n",
        "\n",
        "            a = tokens[i]\n",
        "            b = tokens[i + 1]\n",
        "\n",
        "            if a + ' ' + b in dictionary:\n",
        "                index = tokens.index(a)\n",
        "                tokens.remove(a)\n",
        "                tokens.remove(b)\n",
        "                tokens.insert(index, a + ' ' + b)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Z2y5vpYRlKAe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs"
      ],
      "metadata": {
        "id": "WIBeQE-7wp9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter the text to Tokkenize: \")\n",
        "print(f\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\")"
      ],
      "metadata": {
        "id": "nY0TO_KjrzFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b26727-cf22-4dec-e2bd-f22a5c0683a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to Tokkenize: askriat pasandi per bar waqat karwai karne ke zaroorat hai!\n",
            "\n",
            "\n",
            "The tokenized string is as follow : \n",
            " ['askriat pasandi', 'per', 'bar waqat', 'karwai', 'karne', 'ke', 'zaroorat', 'hai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter the text to Tokkenize: \")\n",
        "print(f\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\")"
      ],
      "metadata": {
        "id": "D_sSUWbBvVSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d3b40c-bde8-4c19-87aa-0bdb82455548"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to Tokkenize: Economic Forum per steal mills ke mehangay damon, tarkin watan aur money laundering per bhi baat hogi.\n",
            "\n",
            "\n",
            "The tokenized string is as follow : \n",
            " ['economic forum', 'per', 'steal mills', 'ke', 'mehangay damon', 'tarkin watan', 'aur', 'money laundering', 'per', 'bhi', 'baat', 'hogi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Enter the text to Tokkenize: \")\n",
        "print(f\"\\n\\nThe tokenized string is as follow : \\n {roman_tokenizer(sentence)}\")"
      ],
      "metadata": {
        "id": "8A0SaBsewWlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cf2352-7494-4dfc-89df-068364edea3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to Tokkenize: Supreme court ke zimma daari hai yeh!\n",
            "\n",
            "\n",
            "The tokenized string is as follow : \n",
            " ['supreme court', 'ke', 'zimma daari', 'hai', 'yeh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Sentences\n",
        "['kiya aqwam mutahidda ka ijlas kal hoga?']\n",
        "['askriat pasandi per bar waqat karwai karne ke zaroorat hai!']\n",
        "['Supreme court ke zimma daari hai yeh!']\n",
        "['Economic Forum per steal mills ke mehangay damon, tarkin watan aur money laundering per bhi baat hogi.']"
      ],
      "metadata": {
        "id": "wteiG0OYr7OK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55342c8-16e8-44a4-f206-d0a75f0277c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Economic Forum per steal mills ke mehangay damon, tarkin watan aur money laundering per bhi baat hogi.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}